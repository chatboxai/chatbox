Adam优化器（Adaptive Moment Estimation）是一种常用的优化算法，常用于训练深度学习模型。其主要思想是在梯度下降的基础上，结合了动量和自适应学习率的思想。Adam优化器的迭代步骤如下：

1. **初始化变量**：
   - 学习率 \(\alpha\)（通常推荐的初始值是0.001）。
   - 一阶矩估计 \(m_0 = 0\)（动量）。
   - 二阶矩估计 \(v_0 = 0\)（平方梯度）。
   - 时间步数 \(t = 0\)。
   - \(\beta_1\) 和 \(\beta_2\) 的默认值一般是 \(\beta_1 = 0.9\) 和 \(\beta_2 = 0.999\)。
   - \(\epsilon\) 是一个小常数（通常设为 \(10^{-8}\)）以避免除零错误。

2. **迭代直到收敛**：
   对于每次迭代 \(t\)（通常是训练的每一个batch）：
 
   a. **计算梯度**：
   \[
   g_t = \nabla_{\theta} J(\theta_t)
   \]
   其中 \(J(\theta_t)\) 是损失函数，\(\theta_t\) 是当前的参数。

   b. **更新一阶矩估计（动量）**：
   \[
   m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
   \]

   c. **更新二阶矩估计（平方梯度）**：
   \[
   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
   \]

   d. **偏差修正**（使得初始值的影响减小）：
   \[
   \hat{m_t} = \frac{m_t}{1 - \beta_1^t}
   \]
   \[
   \hat{v_t} = \frac{v_t}{1 - \beta_2^t}
   \]

   e. **更新参数**：
   \[
   \theta_{t+1} = \theta_t - \frac{\alpha \hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
   \]

3. **循环至结束**：
   重复步骤2，直到满足终止条件（如达到最大迭代次数或损失函数收敛）。

通过这些步骤，Adam优化器能够自适应地调整每个参数的学习率，有效加速收敛并提高训练的稳定性。